import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk import FreqDist
from nltk.metrics import jaccard_distance
from nltk.metrics.distance import edit_distance
from nltk.cluster.util import cosine_distance

# Download necessary resources (if not already downloaded)
nltk.download('punkt')
nltk.download('stopwords')

# Sample sentences
sentence1 = "The quick brown fox jumps over the lazy dog."
sentence2 = "A fast red fox leaps over a sleepy canine."

# Tokenize sentences and remove stopwords
stop_words = set(stopwords.words('english'))

def preprocess(text):
    words = word_tokenize(text)
    words = [word.lower() for word in words if word.isalnum()]
    words = [word for word in words if word not in stop_words]
    return words

words1 = preprocess(sentence1)
words2 = preprocess(sentence2)

# Calculate Jaccard similarity
jaccard_sim = 1 - jaccard_distance(set(words1), set(words2))

# Calculate Levenshtein similarity
levenshtein_sim = 1 - edit_distance(words1, words2) / max(len(words1), len(words2))

# Calculate cosine similarity
fdist1 = FreqDist(words1)
fdist2 = FreqDist(words2)
common_words = set(fdist1.keys()) & set(fdist2.keys())

def vectorize_sentence(sentence, common_words):
    vector = [fdist1[word] for word in common_words]
    return vector

vector1 = vectorize_sentence(words1, common_words)
vector2 = vectorize_sentence(words2, common_words)

cosine_sim = 1 - cosine_distance(vector1, vector2)

print("Jaccard Similarity:", jaccard_sim)
print("Levenshtein Similarity:", levenshtein_sim)
print("Cosine Similarity:", cosine_sim)
